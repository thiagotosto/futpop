{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Polyfit sanity test emitted a warning, most likely due to using a buggy Accelerate backend. If you compiled yourself, see site.cfg.example for information. Otherwise report this to the vendor that provided NumPy.\nRankWarning: Polyfit may be poorly conditioned\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e7e6d75ad5c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#import matplotlib.pyplot as plt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#import seaborn as sns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#%matplotlib inline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdependency\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhard_dependencies\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0m__import__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdependency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mmissing_dependencies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{dependency}: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    284\u001b[0m                     \"that provided NumPy.\\n{}\\n\".format(\n\u001b[1;32m    285\u001b[0m                         error_message))\n\u001b[0;32m--> 286\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0m_mac_os_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Polyfit sanity test emitted a warning, most likely due to using a buggy Accelerate backend. If you compiled yourself, see site.cfg.example for information. Otherwise report this to the vendor that provided NumPy.\nRankWarning: Polyfit may be poorly conditioned\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import configparser\n",
    "\n",
    "class Mongo():\n",
    "\n",
    "    def __init__(self, config_file):\n",
    "        config = configparser.RawConfigParser()\n",
    "        config.read(config_file)\n",
    "\n",
    "        self.server = config.get('mongo', 'server') if 'server' in [i[0] for i in config.items('mongo')] else False\n",
    "        self.db = config.get('mongo', 'db') if 'db' in [i[0] for i in config.items('mongo')] else False\n",
    "        self.host = config.get('mongo', 'host') if 'host' in [i[0] for i in config.items('mongo')] else False\n",
    "        self.port = int(config.get('mongo', 'port')) if 'port' in [i[0] for i in config.items('mongo')] else False\n",
    "        self.user = config.get('mongo', 'user') if 'user' in [i[0] for i in config.items('mongo')] else False\n",
    "        self.password = config.get('mongo', 'password') if 'password' in [i[0] for i in config.items('mongo')] else False\n",
    "\n",
    "        print(self.server)\n",
    "        \n",
    "    def connect(self):\n",
    "        if self.server == 'True':\n",
    "            connection_string_basic = \"mongodb+srv://{user}:{password}@{host}{port}{db}\".format(user=self.user,\n",
    "                                                                                                   password=self.password,\n",
    "                                                                                                   host=self.host,\n",
    "                                                                                                   port=\"{port}\",\n",
    "                                                                                                   db=\"{db}\")\n",
    "        else:\n",
    "            connection_string_basic = \"mongodb://{user}:{password}@{host}{port}{db}\".format(user=self.user,\n",
    "                                                                                            password=self.password,\n",
    "                                                                                            host=self.host,\n",
    "                                                                                            port=\"{port}\",\n",
    "                                                                                            db=\"{db}\")\n",
    "\n",
    "        if self.port:\n",
    "            connection_string_port = connection_string_basic.format(port=\":{}\".format(self.port),\n",
    "                                                                    db=\"{db}\")\n",
    "        else:\n",
    "            connection_string_port = connection_string_basic.format(port=\"\",\n",
    "                                                                    db=\"{db}\")\n",
    "\n",
    "        if self.db:\n",
    "            connection_string = connection_string_port.format(db=\"/{}\".format(self.db))\n",
    "        else:\n",
    "            connection_string = connection_string_port.format(db=\"\")\n",
    "        \n",
    "        print(connection_string)\n",
    "        return MongoClient(connection_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo = Mongo('config.cfg')\n",
    "mongo_client = mongo.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_client.datapop.tweets_to_classify.aggregate([{\"$sample\":{\"size\": 3}}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample = {str(obj['_id']): {i:obj[i] for i in obj if i != '_id'} for obj in mongo_client.datapop.tweets_to_classify.aggregate([{'$sample': {'size': 3}}])}\n",
    "tweets = {str(obj['_id']): {i:obj[i] for i in obj if i != '_id'} for obj in mongo_client.datapop.tweets_classified.find()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_list = [tweets[key] for key in tweets.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = pd.DataFrame.from_dict(tweets_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets.sentiment.value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_positives = df_tweets[df_tweets['sentiment'] == 'positive'][['full_text', 'sentiment', 'rate', 'classified_by']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neutrals = df_tweets[df_tweets['sentiment'] == 'neutral'][['full_text', 'sentiment', 'rate', 'classified_by']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_negatives = df_tweets[df_tweets['sentiment'] == 'negative'][['full_text', 'sentiment', 'rate', 'classified_by']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neutrals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets.rate.value_counts().sort_index().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizando os textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tweet_tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collectTokens(tweets):\n",
    "    list_of_word = []\n",
    "    for tweet, sentiment in tweets:\n",
    "        list_of_word = list_of_word + [(token.lower(), sentiment) for token in tweet_tokenizer.tokenize(tweet)]\n",
    "    \n",
    "    return list_of_word\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df_tweets[['full_text', 'sentiment']].index:\n",
    "    print(df_tweets[['full_text', 'sentiment']].iloc[i]['full_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = collectTokens([(df_tweets[['full_text', 'sentiment']].iloc[i]['full_text'],\n",
    "                         df_tweets[['full_text', 'sentiment']].iloc[i]['sentiment']) for i in df_tweets[['full_text',\n",
    "                                                                                                        'sentiment']].index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removendo Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portuguese_stop_words = stopwords.words('portuguese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopwords(tokens):\n",
    "    return [token for token in tokens if token[0] not in portuguese_stop_words + ['pra', 'vc', 'q', 'vcs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_no_stopwords = removeStopwords(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens_no_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_no_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removendo Menções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeMentions(tokens):\n",
    "    mentions_regex = re.compile('[^@]')\n",
    "    return list(filter(lambda tweet: mentions_regex.match(tweet[0]), tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_no_mentions = removeMentions(tokens_no_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens_no_mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_no_mentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removendo links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeLinks(tokens):\n",
    "    links_regex = re.compile('^(?!http).*')\n",
    "    return list(filter(lambda tweet: links_regex.match(tweet[0]), tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_no_links = removeLinks(tokens_no_mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens_no_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_no_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removendo Números e placares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeNumbersAndScores(tokens):\n",
    "    numbers_regex = re.compile('(([0-9]x[0-9])|(^x$)|(^[0-9]+$))')\n",
    "    return [token for token in tokens if token not in list(filter(lambda tweet: numbers_regex.match(tweet[0]), tokens))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_no_numbers_scores = removeNumbersAndScores(tokens_no_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens_no_numbers_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_no_numbers_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Pontuação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removePontuation(tokens):\n",
    "    pontuation_regex = re.compile('[^!\\.?:\\[\\],\\{\\}\\\\\\/;\"\\(\\)]')\n",
    "    return list(filter(lambda tweet: pontuation_regex.match(tweet[0]), tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_no_pontuation = removePontuation(tokens_no_numbers_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens_no_pontuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_no_pontuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removendo Nome de Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeTeams(tokens):\n",
    "    teams_list = ['fluminense',\n",
    "                  'flamengo',\n",
    "                  'botafogo',\n",
    "                  'vasco',\n",
    "                  'são paulo',\n",
    "                  'palmeiras',\n",
    "                  'santos',\n",
    "                  'corinthians',\n",
    "                  'cruzeiro',\n",
    "                  'atlético mg',\n",
    "                  'atlético mineiro',\n",
    "                  'atletico mg',\n",
    "                  'atletico mineiro',\n",
    "                  'internacional',\n",
    "                  'gremio',\n",
    "                  'chapecoense',\n",
    "                  'avai',\n",
    "                  'csa',\n",
    "                  'bahia',\n",
    "                  'goias',\n",
    "                  'athletico pr',\n",
    "                  'athletico paranaense',\n",
    "                  'atletico paranaense',\n",
    "                  'atlético paranaense'\n",
    "                  'ceara',\n",
    "                  'fortaleza'\n",
    "                 ]\n",
    "    return [token for token in tokens if token[0] not in teams_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_no_teams = removeTeams(tokens_no_pontuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(token_no_teams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_no_teams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RSLPStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(tokens):\n",
    "    stemmer = RSLPStemmer()\n",
    "    \n",
    "    return [(stemmer.stem(token[0]), token[1]) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_stemmed = stem(token_no_teams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe de Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_token = pd.DataFrame(token_no_teams, columns =['token', 'sentiment']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_token['token'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_token_grouped = df_token.groupby(['token', 'sentiment']).size().unstack(fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_token_grouped['total'] = df_token_grouped['positive']\\\n",
    "                            + df_token_grouped['neutral']\\\n",
    "                            + df_token_grouped['negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_token_grouped.iloc[1000:1300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_token_grouped.sort_values('total', ascending=False)[['positive', 'neutral', 'negative']].iloc[:20].plot.bar(figsize=(15,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_token_grouped.sort_values('positive', ascending=False)[['positive', 'neutral', 'negative']].iloc[:20].plot.bar(figsize=(15,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_token_grouped.sort_values('neutral', ascending=False)[['positive', 'neutral', 'negative']].iloc[:20].plot.bar(figsize=(15,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_token_grouped.sort_values('negative', ascending=False)[['positive', 'neutral', 'negative']].iloc[:20].plot.bar(figsize=(15,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_token_grouped['sentiment'] = df_token_grouped.apply(lambda r: 'positive' if r.max() == r.positive \n",
    "                                                       else 'negative' if r.max() == r.negative\n",
    "                                                       else 'neutral',\n",
    "                                                       axis=1)\n",
    "#df_token_grouped.loc['#'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_token_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Dataframe de Tokens\n",
    "\n",
    "df_token = pd.DataFrame(token_stemmed, columns =['token', 'sentiment']) \n",
    "\n",
    "df_token['token'].describe()\n",
    "\n",
    "df_token_grouped = df_token.groupby(['token', 'sentiment']).size().unstack(fill_value=0)\n",
    "\n",
    "df_token_grouped['total'] = df_token_grouped['positive']\\\n",
    "                            + df_token_grouped['neutral']\\\n",
    "                            + df_token_grouped['negative']\n",
    "\n",
    "df_token_grouped['sentiment'] = df_token_grouped.apply(lambda r: 'positive' if r.max() == r.positive \n",
    "                                                       else 'negative' if r.max() == r.negative\n",
    "                                                       else 'neutral',\n",
    "                                                       axis=1)\n",
    "\n",
    "df_token_grouped.iloc[1000:1300]\n",
    "\n",
    "df_token_grouped.sort_values('total', ascending=False)[['positive', 'neutral', 'negative']].iloc[:20].plot.bar(title='Total', figsize=(15,10))\n",
    "\n",
    "df_token_grouped.sort_values('positive', ascending=False)[['positive', 'neutral', 'negative']].iloc[:20].plot.bar(title='Positive', figsize=(15,10))\n",
    "\n",
    "df_token_grouped.sort_values('neutral', ascending=False)[['positive', 'neutral', 'negative']].iloc[:20].plot.bar(title='Neutral', figsize=(15,10))\n",
    "\n",
    "df_token_grouped.sort_values('negative', ascending=False)[['positive', 'neutral', 'negative']].iloc[:20].plot.bar(title='Negative', figsize=(15,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#def predict():\n",
    "df_token_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv =  CountVectorizer(lowercase=True,stop_words=portuguese_stop_words + ['pra', 'vc', 'q', 'vcs'],ngram_range = (1,1),tokenizer = tweet_tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_counts = cv.fit_transform(df_tweets['full_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf=TfidfVectorizer()\n",
    "text_tf= tf.fit_transform(df_tweets['full_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "BoW_X_train, BoW_X_test, BoW_y_train, BoW_y_test = train_test_split(\n",
    "    text_counts, df_tweets['sentiment'], test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "TF_X_train, TF_X_test, TF_y_train, TF_y_test = train_test_split(\n",
    "    text_tf, df_tweets['sentiment'], test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultinomialNB "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "# Model Generation Using Multinomial Naive Bayes\n",
    "clf = MultinomialNB().fit(BoW_X_train, BoW_y_train)\n",
    "predicted= clf.predict(BoW_X_test)\n",
    "print(\"MultinomialNB Accuracy:\",metrics.accuracy_score(BoW_y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "# Model Generation Using Multinomial Naive Bayes\n",
    "clf = MultinomialNB().fit(TF_X_train, TF_y_train)\n",
    "predicted= clf.predict(TF_X_test)\n",
    "print(\"MultinomialNB Accuracy:\",metrics.accuracy_score(TF_y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(\"testando\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeLinks(tokens):\n",
    "    links_regex = re.compile('^(?!http).*')\n",
    "    return list(filter(lambda tweet: links_regex.match(tweet[0]), tokens))\n",
    "\n",
    "tokens_no_links = removeLinks(tokens_no_mentions)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
